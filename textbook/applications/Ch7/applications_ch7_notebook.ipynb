{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "id": "01bd1805",
      "cell_type": "markdown",
      "source": "(ch7:code_repr)=\n# Code Reproducibility",
      "metadata": {}
    },
    {
      "id": "c0611f98",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.simulations import sbm\nimport numpy as np\nfrom graphbook_code import dcsbm, generate_dcsbm_pmtx, \\\n                           generate_sbm_pmtx\n\n\nn = 100  # the number of nodes\n# human brains have homophilic block structure\nBhum = np.array([[0.2, 0.02], [0.02, 0.2]])\n# alien brains add degree-correction\ntheta_alien = np.tile(np.linspace(1.5, 0.5, n // 2), 2)\n\n# generate human and alien brain network\nnp.random.seed(0)\nA_human, z = sbm([n // 2, n // 2], Bhum, return_labels=True)\nA_alien = dcsbm(z, theta_alien, Bhum)\n\nPhum = generate_sbm_pmtx(z, Bhum)\nPalien = generate_dcsbm_pmtx(z, theta_alien, Bhum)",
      "outputs": []
    },
    {
      "id": "3bfde84e",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from scipy.linalg import orthogonal_procrustes\nfrom graspologic.embed import AdjacencySpectralEmbed as ase\n\nd = 2\n# estimate latent positions for alien and human networks\nXhat_human = ase(n_components=d).fit_transform(A_human)\nXhat_alien = ase(n_components=d).fit_transform(A_alien)\n# estimate best possible orthogonal transform of Xhat_alien to Xhat_human by \n# solving orthogonal procrustes problem\nW = orthogonal_procrustes(Xhat_alien, Xhat_human)[0]\nobserved_norm = np.linalg.norm(Xhat_human - Xhat_alien @ W, ord=\"fro\")",
      "outputs": []
    },
    {
      "id": "4218ca70",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.simulations import rdpg\n\ndef generate_synthetic_networks(X):\n    \"\"\"\n    A function which generates two synthetic networks with\n    same latent position matrix X.\n    \"\"\"\n    A1 = rdpg(X, directed=False, loops=False)\n    A2 = rdpg(X, directed=False, loops=False)\n    return A1, A2\n\nAp, App = generate_synthetic_networks(Xhat_human)",
      "outputs": []
    },
    {
      "id": "50c08055",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def compute_latent(A, d):\n    \"\"\"\n    A function which returns the latent position estimate\n    for an adjacency matrix A.\n    \"\"\"\n    return ase(n_components=d).fit_transform(A)\n\nXhat_p = compute_latent(Ap, d)\nXhat_pp = compute_latent(App, d)",
      "outputs": []
    },
    {
      "id": "c7aa1694",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def compute_norm_orth_proc(A, B):\n    \"\"\"\n    A function which finds the best orthogonal transform \n    of B onto A, and then computes and returns the norm.\n    \"\"\"\n    R = orthogonal_procrustes(B, A)[0]\n    return np.linalg.norm(A - B @ R)\n\nnorm_null = compute_norm_orth_proc(Xhat_p, Xhat_pp)",
      "outputs": []
    },
    {
      "id": "ed2d67f9",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def parametric_resample(A1, A2, d, nreps=100):\n    \"\"\"\n    A function to generate samples of the null distribution under H0\n    using parametric resampling.\n    \"\"\"\n    null_norms = np.zeros(nreps)\n    Xhat1 = compute_latent(A1, d)\n    for i in range(0, nreps):\n        Ap, App = generate_synthetic_networks(Xhat1)\n        Xhat_p = compute_latent(Ap, d)\n        Xhat_pp = compute_latent(App, d)\n        null_norms[i] = compute_norm_orth_proc(Xhat_p, Xhat_pp)\n    return null_norms\n\nnreps = 100\nnull_norms = parametric_resample(A_alien, A_human, 2, nreps=nreps)",
      "outputs": []
    },
    {
      "id": "f92b9e44",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "pval = ((null_norms >= observed_norm).sum() + 1)/(nreps + 1)\nprint(f\"estimate of p-value: {pval:.5f}\")\n# estimate of p-value: 0.00990",
      "outputs": []
    },
    {
      "id": "8f74f47b",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.inference import latent_position_test\n\nnreps = 100 # the number of null replicates\nlpt = latent_position_test(A_human, A_alien, n_bootstraps = nreps, n_components=d, workers=-1)\nprint(\"estimate of p-value: {:.5f}\".format(lpt[1]))\n# estimate of p-value: 0.00990",
      "outputs": []
    },
    {
      "id": "589a1c38",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# generate a new human brain network with same block matrix\nA_human2 = sbm([n // 2, n // 2], Bhum)\n\nlpt_hum2hum = latent_position_test(A_human, A_human2, n_bootstraps=nreps, n_components=d, workers=-1)\nprint(\"estimate of p-value: {:.5f}\".format(lpt_hum2hum[1]))\n# estimate of p-value: 0.386",
      "outputs": []
    },
    {
      "id": "3b63e004",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "ncoins = 300  # the number of coins in each container\n\n# the probabilities from container 1 landing on heads\n# with a much larger variance\npi1 = np.random.beta(a=4, b=4, size=ncoins)\n\n# the probabilities of container 2 landing on heads,\n# with a much smaller variance\npi2 = np.random.beta(a=15, b=15, size=ncoins)",
      "outputs": []
    },
    {
      "id": "b54be586",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.inference import latent_distribution_test\n\nnreps = 100\napproach = 'mgc'  # the strategy for the latent distribution test\nldt_dcorr = latent_distribution_test(A_human, A_alien, test=approach, metric=\"euclidean\", n_bootstraps=nreps, workers=-1)\nprint(\"estimate of p-value: {:.5f}\".format(ldt_dcorr.pvalue))\n# estimate of p-value: 0.00990",
      "outputs": []
    },
    {
      "id": "22fe86a8",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "print(\"estimate of p-value: {:.4f}\".format(ldt_dcorr[1]))",
      "outputs": []
    },
    {
      "id": "d6250a87",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\nfrom graspologic.simulations import sbm\n\nns = [45, 30, 25]  # number of exits\n\nstates = [\"NY\", \"NJ\", \"PA\"]\n# z is a column vector indicating which state each exit is in\nz = np.repeat(states, ns)\n\nBnight = np.array([[.3, .2, .05], [.2, .3, .05], [.05, .05, .3]])\nBday = Bnight*2  # day time block matrix is generally 50% more than night\n\n# people tend to commute from New Jersey to New York during the day\n# at anomalously high rates\nBday[0, 1] = .5; Bday[1,0] = .5\n\nnp.random.seed(0)\nAnight = sbm(ns, Bnight)\nAday = sbm(ns, Bday)",
      "outputs": []
    },
    {
      "id": "e844ad63",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from scipy.stats import fisher_exact\n\nK = 3\nPvals = np.empty((K, K))\n# fill matrix with NaNs\nPvals[:] = np.nan\n\n# get the indices of the upper triangle of Aday\nupper_tri_idx = np.triu_indices(Aday.shape[0], k=1)\n# create a boolean array that is nxn\nupper_tri_mask = np.zeros(Aday.shape, dtype=bool)\n# set indices which correspond to the upper triangle to True\nupper_tri_mask[upper_tri_idx] = True\n\nfor k in range(0, K):\n    for l in range(k, K):\n        comm_mask = np.outer(z == states[k], z == states[l])\n        table = [[Aday[comm_mask & upper_tri_mask].sum(),\n                 (Aday[comm_mask & upper_tri_mask] == 0).sum()],\n                 [Anight[comm_mask & upper_tri_mask].sum(),\n                 (Anight[comm_mask & upper_tri_mask] == 0).sum()]]\n        Pvals[k,l] = fisher_exact(table)[1]",
      "outputs": []
    },
    {
      "id": "a43dca65",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\nfrom graspologic.simulations import er_np\nimport seaborn as sns\nfrom scipy.stats import binomtest\n\nncoins = 5000 # the number of coins\np = 0.5  # the true probability\nn = 500  # the number of flips\n\n# the number of heads from each experiment\nexperiments = np.random.binomial(n, p, size=ncoins)\n\n# perform binomial test to see if the number of heads we obtain supports that the\n# true probabiily is 0.5\npvals = [binomtest(nheads_i, n, p=p).pvalue for nheads_i in experiments]",
      "outputs": []
    },
    {
      "id": "20c45ebd",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from statsmodels.stats.multitest import multipletests\n\nalpha = 0.05  # the desired alpha of the test\n_, adj_pvals, _, _ = multipletests(pvals, alpha=alpha, method=\"holm\")",
      "outputs": []
    },
    {
      "id": "82e350ee",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.utils import symmetrize\n\nPvals_adj = multipletests(Pvals.flatten(), method=\"holm\")[1].reshape(K, K)\nPvals_adj = symmetrize(Pvals_adj, method=\"triu\")",
      "outputs": []
    },
    {
      "id": "11bf606a",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "pval_dif = Pvals_adj.min()\nprint(f\"p-value of block matrix difference: {pval_dif:.4f}\")\n# p-value of block matrix difference: 0.0000",
      "outputs": []
    },
    {
      "id": "b1bcda39",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.inference import group_connection_test\n\nstat, pval_diff_rescale, misc = group_connection_test(Aday, Anight,\n    labels1=z, labels2=z, density_adjustment=True)\nPval_adj_rescaled = np.array(misc[\"corrected_pvalues\"])\nprint(f\"p-value of block matrix difference, after rescaling: {pval_diff_rescale:.4f}\")\n# p-value of block matrix difference, after rescaling: 0.0005",
      "outputs": []
    },
    {
      "id": "98fd86ac",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\n\nA = np.array([\n    [1,1,1,1],\n    [2,2,2,2],\n    [3,3,3,3],\n    [4,4,4,4]\n])\n\nP = np.array([\n    [0,1,0,0],\n    [1,0,0,0],\n    [0,0,1,0],\n    [0,0,0,1]\n])\n\nrow_reordering = P.T @ A",
      "outputs": []
    },
    {
      "id": "dc8f5e27",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "B = np.array([\n    [1,2,3,4],\n    [1,2,3,4],\n    [1,2,3,4],\n    [1,2,3,4]\n])\n\ncolumn_reordering = B @ P",
      "outputs": []
    },
    {
      "id": "7b6b9bdc",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "C = np.array([\n    [1,1,1,1],\n    [1,0,0,0],\n    [1,0,0,0],\n    [1,0,0,0]\n])\n\nrow_reordering_C = P.T @ C\nrow_column_reordering = row_reordering_C @ P",
      "outputs": []
    },
    {
      "id": "550be8ce",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "insta = np.array([\n    [0,1,1,0],\n    [1,0,0,1],\n    [1,0,0,1],\n    [0,1,1,0]\n])\n\nfacebook_permuted = np.array([\n    [0,1,0,1],\n    [1,0,1,0],\n    [0,1,0,1],\n    [1,0,1,0]\n])\n\n# the permutation to unshuffle the facebook\n# permuted adjacency matrix\nPu = np.array([\n    [1,0,0,0],\n    [0,1,0,0],\n    [0,0,0,1],\n    [0,0,1,0]\n])\n\nfb_unpermuted = Pu.T @ facebook_permuted @ Pu",
      "outputs": []
    },
    {
      "id": "3e29bb73",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def make_random_permutation(n, random_seed=0):\n    \"\"\"\n    A function that generates a random permutation matric $P$ for n elements.\n    \n    1. Generate indices from 0 to n-1\n    2. shuffle those indices\n    3. Place 1s in the matrix P at the positions defined by the shuffled indices.\n    \"\"\"\n    rng = np.random.default_rng(seed=random_seed)\n    starting_indices = np.arange(n)\n    destination_indices = rng.permutation(n)\n    P = np.zeros(shape=(n,n))\n    P[destination_indices, starting_indices] = 1\n    return P",
      "outputs": []
    },
    {
      "id": "c6b87d26",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.simulations import er_np\n\nn = 12\np = 0.5\n\nnp.random.seed(0)\nA = er_np(n=n, p=p)\n# make a random permutation matrix\nP = make_random_permutation(n)\nB = P.T @ A @ P\ndisagreements = np.linalg.norm(A - B)**2",
      "outputs": []
    },
    {
      "id": "3d9d74e8",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.match import graph_match\n\ngmp = graph_match(A,B, n_init=10, rng=0)",
      "outputs": []
    },
    {
      "id": "01e0c9e3",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def make_unshuffler(destination_indices):\n    \"\"\"\n    A function which creates a permutation matrix P from a given permutation of the nodes.\n    \"\"\"\n    n = len(destination_indices)\n    Pu = np.zeros((n, n))\n    starting_indices = np.arange(n)\n    Pu[destination_indices, starting_indices] = 1\n    return Pu\n\nPu = make_unshuffler(gmp.indices_B)\nB_unshuffled = Pu.T @ B @ Pu\ndisagreements = np.linalg.norm(A - B_unshuffled)**2\nprint(f\"Disagreements: {int(disagreements):d}\")\n# Disagreements: 0.0",
      "outputs": []
    },
    {
      "id": "f59e0058",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def match_ratio(P, Pu):\n    n = P.shape[0]  # the number of nodes\n    return (np.diag(Pu @ P) == 1).sum()/n\n\nprint(f\"match ratio: {match_ratio(P, Pu):.3f}\")\n# match ratio: 1.000",
      "outputs": []
    },
    {
      "id": "443de4fc",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.simulations import sbm_corr\n\nn_per_block = 75\nn_blocks = 3\nblock_members = np.repeat(n_per_block, repeats=n_blocks)\nn_nodes = block_members.sum()\nrho = 0.5\nblock_probs = np.array(\n    [[0.7, 0.1, 0.4], \n     [0.1, 0.3, 0.1], \n     [0.4, 0.1, 0.7]]\n)\n\nnp.random.seed(0)\nA1, A2 = sbm_corr(block_members, block_probs, rho)\ndisagreements = np.linalg.norm(A1 - A2)**2\nprint(f\"Disagreements (Unshuffled): {int(disagreements):d}\")\n# Disagreements (Unshuffled): 8041",
      "outputs": []
    },
    {
      "id": "35002f3b",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "P = make_random_permutation(n_nodes)\nA2_shuffle = P.T @ A2 @ P\ndisagreements_shuffled = np.linalg.norm(A1 - A2_shuffle)**2\nprint(f\"Disagreements (Shuffled): {int(disagreements_shuffled):d}\")\n# Disagreements (Shuffled): 22201",
      "outputs": []
    },
    {
      "id": "dc761bcd",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# fit with A and shuffled B\ngm = graph_match(A1, A2_shuffle, rng=0)\n\n# obtain unshuffled version of the shuffled B\nP_unshuffle_noseed = make_unshuffler(gm.indices_B)\nA2_unshuffle_noseed = P_unshuffle_noseed.T @ A2_shuffle @ P_unshuffle_noseed\n\n# compute the match ratio\nmatch_ratio_noseed = match_ratio(P, P_unshuffle_noseed)\nprint(f\"Match Ratio, no seeds: {match_ratio_noseed:.3f}\")\n# Match Ratio, no seeds: 0.004\n\ndisagreements_noseed = np.linalg.norm(A1 - A2_unshuffle_noseed)**2\nprint(f\"Disagreements, no seeds: {int(disagreements_noseed):d}\")\n# Disagreements, no seeds: 12810",
      "outputs": []
    },
    {
      "id": "140a3ccf",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def gen_seeds(P, n_seeds, random_seed=0):\n    \"\"\"\n    A function to generate n_seeds seeds for a pair of matrices A1 and P^TA2P\n    which are initially matched, but P has been applied to permute the nodes\n    of A2.\n    \"\"\"\n    rng = np.random.default_rng(seed=random_seed)\n    n = P.shape[0]\n    # obtain n_seeds random seeds from 1:n\n    seeds = rng.choice(n, size=n_seeds, replace=False)\n    # use the permutation matrix to find where each seed was permuted to\n    seeds_permuted = [np.where(P[i, :] == 1)[0] for i in seeds]\n    return (seeds, seeds_permuted)",
      "outputs": []
    },
    {
      "id": "751f1061",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "nseeds = 10  # the number of seeds to use\n# select ten nodes at random from A which will serve as seeds\n\n# obtain seeds for nodes of A1 with nodes of A2 after shuffling\nseedsA1, seedsA2_shuffled = gen_seeds(P, nseeds)\n\n# run SGM with A1 and shuffled A2, but provide the seed nodes from A as ref_seeds\n# and the corresponding position of these seed nodes after shuffling as permuted_seeds\nsgm = graph_match(A1, A2_shuffle, partial_match=(seedsA1, seedsA2_shuffled), rng=0)\nP_unshuffle_seeds = make_unshuffler(sgm.indices_B)\nA2_unshuffle_seeds = P_unshuffle_seeds.T @ A2_shuffle @ P_unshuffle_seeds\n\nmatch_ratio_seeds = match_ratio(P, P_unshuffle_seeds)\nprint(f\"Match Ratio, seeds: {match_ratio_seeds:.3f}\")\n# Match Ratio with seeds: 1.000\n\ndisagreements_seeds = np.linalg.norm(A1 - A2_unshuffle_seeds)**2\nprint(f\"Disagreements, seeds: {int(disagreements_seeds):d}\")\n# Disagreements, seeds: 8041",
      "outputs": []
    },
    {
      "id": "55a69c20",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.utils import remove_vertices\n\nnremove = 25\n\n# nodes to remove from A2\nn_nodes_A2_N = n_nodes - nremove*n_blocks\nbase_range = np.arange(n_per_block - nremove, n_per_block)\nblock_offsets = np.array([0, 75, 150])\n\n# repeat a base range for each block and add block offsets\nnodes_to_remove = np.repeat(base_range, len(block_offsets)) \nnodes_to_remove += np.tile(block_offsets, nremove)\nN = np.setdiff1d(np.arange(n_nodes), nodes_to_remove)\n\n# use the remove_vertices function to compute\n# the subnetwork induced by the nodes nodes_to_retain\nA2_N = remove_vertices(A2, nodes_to_remove)",
      "outputs": []
    },
    {
      "id": "1f83289e",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "A1_N = remove_vertices(A1, nodes_to_remove)",
      "outputs": []
    },
    {
      "id": "b9f89c9b",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "A2_N_padded = np.pad(\n    A2_N, \n    pad_width=[(0,nremove*n_blocks), (0, nremove*n_blocks)]\n)",
      "outputs": []
    },
    {
      "id": "925f4955",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "nseeds_padded = 10\n\nnp.random.seed(0)\n# obtain which nodes of A2 will be the seeds to use, from the retained nodes in the network\nseeds_A2_N = np.random.choice(n_nodes_A2_N, size=nseeds_padded, replace=False)\n\n# obtain the nodes in A1\nseeds_A1 = N[seeds_A2_N]\n\n# run SGM with A1 and the padded network A2\n# since we didn't shuffle A(2),r, we do not need\n# to worry about permuting the seeds\nsgm_naive = graph_match(A1, A2_N, partial_match=(seeds_A1, seeds_A2_N),\n                        padding=\"naive\", rng=0, n_init=5)\n\n# unshuffle A2_N using indices_B\nP_unshuffle = make_unshuffler(sgm_naive.indices_B)\nA2_N_unshuffle_seeds_naive = P_unshuffle.T @ A2_N @ P_unshuffle\n\nA2_naive_full = np.zeros(A1.shape)\nA2_naive_full[np.ix_(sgm_naive.indices_A, sgm_naive.indices_A)] = A2_N_unshuffle_seeds_naive",
      "outputs": []
    },
    {
      "id": "f13392e9",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "A2_naive_full = np.zeros(A1.shape)\nA2_naive_full[np.ix_(sgm_naive.indices_A, sgm_naive.indices_A)] = A2_N_unshuffle_seeds_naive",
      "outputs": []
    },
    {
      "id": "880b7ed4",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "A1_induced = remove_vertices(A1, nodes_to_remove)\ndisagreements_naive = np.linalg.norm(A1_induced - A2_N_unshuffle_seeds_naive)**2\nprint(f\"Disagreements, naive padding: {int(disagreements_naive):d}\")\n# Disagreements, adopted padding: 9285",
      "outputs": []
    },
    {
      "id": "55ecbbf0",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "A1tilde = 2 * A1 - np.ones(A1.shape[0])\nA2tilde_N = 2*A2_N - np.ones(A2_N.shape[0])\nA2tilde_N_padded = np.pad(A2tilde_N, [(0,nremove*n_blocks), (0, nremove*n_blocks)])",
      "outputs": []
    },
    {
      "id": "ec21117e",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# run SGM with A1 and A2[N] with nodes removed\nsgm_adopted = graph_match(A1, A2_N, partial_match=(seeds_A1, seeds_A2_N), padding=\"adopted\", rng=0, n_init=5)\n\n# unshuffle A2[N] using the permutation identified\nP_unshuffle_ad = make_unshuffler(sgm_adopted.indices_B)\nA2_N_unshuffle_seeds_adopted = P_unshuffle_ad.T @ A2_N @ P_unshuffle_ad\n\nA2_adopted_full = np.zeros(A1.shape)\nA2_adopted_full[np.ix_(sgm_adopted.indices_A, sgm_adopted.indices_A)] = A2_N_unshuffle_seeds_adopted",
      "outputs": []
    },
    {
      "id": "9f1a44fb",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "match_ratio_adopted = match_ratio(np.eye(A1_induced.shape[0]), P_unshuffle_ad)\nprint(f\"Match Ratio, adopted padding: {match_ratio_adopted:.3f}\")\n# Match Ratio, adopted padding: 0.887\n\ndisagreements_adopted = np.linalg.norm(A1_induced - A2_N_unshuffle_seeds_adopted)**2\nprint(f\"Disagreements, adopted padding: {int(disagreements_adopted):d}\")\n# Disagreements, adopted padding: 4186",
      "outputs": []
    }
  ]
}