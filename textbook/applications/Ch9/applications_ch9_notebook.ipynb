{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "id": "60065ffb",
      "cell_type": "markdown",
      "source": "(ch9:code_repr)=\n# Code Reproducibility",
      "metadata": {}
    },
    {
      "id": "e0d0e6cc",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from torch_geometric.datasets import MoleculeNet\n\ndataset = MoleculeNet(root='data/clintox', name='ClinTox')\nprint(f'Dataset: {dataset}\\nNumber of molecules/graphs: {len(dataset)}\\nNumber of classes: {dataset.num_classes}')",
      "outputs": []
    },
    {
      "id": "9ec2a8d0",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "mols = dataset[26], dataset[83]\nfor m in mols:\n    print(m.smiles)",
      "outputs": []
    },
    {
      "id": "cc0ff17e",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from rdkit import Chem\nfrom rdkit.Chem.Draw import rdMolDraw2D\nfrom IPython.display import SVG\n\nsmiles = [Chem.MolFromSmiles(m.smiles) for m in mols]\nd2d = rdMolDraw2D.MolDraw2DSVG(600,280,300,280)\nd2d.drawOptions().addAtomIndices = True\nd2d.DrawMolecules(smiles)\nd2d.FinishDrawing()\nSVG(d2d.GetDrawingText())",
      "outputs": []
    },
    {
      "id": "86d42402",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "for i,m in enumerate(mols):\n    print(f'Molecule {i+1}: Number of atoms={m.x.shape[0]}, Features per atom={m.x.shape[1]}')",
      "outputs": []
    },
    {
      "id": "a4f7fb1e",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "d2d = rdMolDraw2D.MolDraw2DSVG(600,280,300,280)\nd2d.drawOptions().addBondIndices = True\nd2d.DrawMolecules(smiles)\nd2d.FinishDrawing()\nSVG(d2d.GetDrawingText())",
      "outputs": []
    },
    {
      "id": "04c75b86",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\n\n_process = lambda x: [e[0] for e in np.split(x, 2)]\ndef adj_from_edgelist(molecule):\n    \"\"\"\n    A function that takes a molecule edgelist and produces an adjacency matrix.\n    \"\"\"\n    # the number of nodes is the number of atoms (rows of .x attribute)\n    n = molecule.x.shape[0]\n    # the adjacency matrix is n x n\n    A = np.zeros((n, n))\n    edgelist = m.edge_index.numpy()\n    # loop over the edges e_k, and for each edge, unpack the \n    # nodes that are incident it. for this pair of nodes, \n    # change the adjacency matrix entry to 1\n    for e_k, (i, j) in enumerate(zip(*_process(edgelist))):\n        A[i, j] = 1\n    return A",
      "outputs": []
    },
    {
      "id": "fbbfad4f",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graphbook_code import heatmap\n\nfor m_i, m in enumerate(mols):\n    A = adj_from_edgelist(m)\n    heatmap(A)",
      "outputs": []
    },
    {
      "id": "5df18976",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import torch\n# for notebook reproducibility\ntorch.manual_seed(12345)\n\ndataset = dataset.shuffle()\n\ntrain_dataset = dataset[:-150]\ntest_dataset = dataset[-150:]\n\nprint(f'Number of training networks: {len(train_dataset)}')\nprint(f'Number of test networks: {len(test_dataset)}')",
      "outputs": []
    },
    {
      "id": "3a65e580",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from torch_geometric.loader import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)",
      "outputs": []
    },
    {
      "id": "7c7a779d",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "for step, data in enumerate(iter(train_loader)):\n    print(f'Step {step + 1}:')\n    print(f'Number of networks in the current batch: {data.num_graphs}')\n    print(data)",
      "outputs": []
    },
    {
      "id": "4d86bb41",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from torch import nn\nfrom torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool\n\ntorch.manual_seed(12345)\nclass GCN(nn.Module):\n    def __init__(self, hidden_channels):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n        self.lin = Linear(hidden_channels, dataset.num_classes, bias=False)\n\n    def forward(self, x, edge_index, batch):\n\n        # 1. Obtain node embeddings via convolutional layers\n        x = self.conv1(x, edge_index)\n        x = x.relu()\n        x = self.conv2(x, edge_index)\n        x = x.relu()\n        x = self.conv3(x, edge_index)\n\n        # 2. Readout layer to produce network embedding\n        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n\n        # 3. Apply a prediction classifier to the network embedding\n        x = self.lin(x)\n\n        return x\n\nmodel = GCN(hidden_channels=64)\nprint(model)",
      "outputs": []
    },
    {
      "id": "959b3507",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "model = GCN(hidden_channels=64)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ncriterion = torch.nn.CrossEntropyLoss()\n\ndef train():\n    model.train()\n    for data in train_loader:  # Iterate in batches over the training dataset.\n        out = model(data.x.float(), data.edge_index, data.batch)  # Perform a single forward pass.\n        # Handle a pyg bug where last element in batch may be all zeros and excluded in the model output.\n        # https://github.com/pyg-team/pytorch_geometric/issues/1813\n        num_batch = out.shape[0]\n        loss = criterion(out, data.y[:num_batch, 0].long())  # Compute the loss.\n        \n        loss.backward()  # Derive gradients.\n        optimizer.step()  # Update parameters based on gradients.\n        optimizer.zero_grad()  # Clear gradients.\n\ndef test(loader):\n        model.eval()\n        correct = 0\n        for data in loader:  # Iterate in batches over the training/test dataset.\n            out = model(data.x.float(), data.edge_index, data.batch)  \n            pred = out.argmax(dim=1)  # Use the class with highest probability.\n            num_batch = pred.shape[0]\n            correct += int((pred == data.y[:num_batch, 0]).sum())  # Check against ground-truth labels.\n        return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n\nR = 10  # number of epochs\nfor epoch in range(0, R):\n    train()\n    train_acc = test(train_loader)\n    test_acc = test(test_loader)\n    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')",
      "outputs": []
    },
    {
      "id": "e1ffa26a",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\n\n# define the node names\nnode_names = np.array([\"SI\", \"MH\", \"BK\", \"Q\", \"BX\"])\n# define the adjacency matrix\nA = np.array([[0,0,1,0,0],  # Staten Island neighbors Brooklyn\n              [0,0,1,1,1],  # Manhattan Neighbors all but Staten Island\n              [1,1,0,1,0],  # Brooklyn neighbors all but Bronx\n              [0,1,1,0,1],  # Queens neighbors all but Staten Island\n              [0,1,0,1,0]]) # Bronx neighbors Manhattan and Queens",
      "outputs": []
    },
    {
      "id": "205ec82b",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# compute the degree of each node\ndi = A.sum(axis=0)\n# the probability matrix is the adjacency divided by\n# degree of the starting node\nP = (A / di).T",
      "outputs": []
    },
    {
      "id": "784033a9",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "x0 = np.array([0,1,0,0,0])  # x vector indicating we start at MH\nps0 = P.T @ x0  # p vector for timestep 1 starting at node MH at time 0\n# choose the next node using the probability vector we calculated\nnext_node = np.random.choice(range(0, len(node_names)), p=ps0)\nprint(f\"Next node: {node_names[next_node]:s}\")",
      "outputs": []
    },
    {
      "id": "f8647b67",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "p = 5  # return parameter\nq = 1/2  # in-out parameter\nbias_vector = np.ones(len(node_names))\nbias_vector[node_names == \"BX\"] = 1/q\nbias_vector[node_names == \"BK\"] = 1/p",
      "outputs": []
    },
    {
      "id": "1a59a20c",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "xt = [0, 1, 0, 0, 0]  # starting vector at MH\npst = P.T @ xt  # probability vector is Pt*x",
      "outputs": []
    },
    {
      "id": "65fe2626",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "bias_factors = pst*bias_vector",
      "outputs": []
    },
    {
      "id": "3eda3d91",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "biased_pst = bias_factors/bias_factors.sum()",
      "outputs": []
    },
    {
      "id": "444b9411",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# choose the next node using the second-order biased transition probability\nnext_node = np.random.choice(range(0, len(node_names)), p=biased_pst)\nprint(f\"Next node: {node_names[next_node]:s}\")",
      "outputs": []
    },
    {
      "id": "bb4bb17e",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graphbook_code import dcsbm\n\nnk = 100  # 100 nodes per community\nzs = np.repeat([1, 2], nk)\nB = np.array([[0.6, 0.3], [0.3, 0.6]])\ntheta = b = np.repeat([1, .2, 1, .2], nk // 2)\ndeg_map = {1: \"Core\", 0.2: \"Per.\"}\n\nzs_deg = [f\"{deg_map[theta[i]]:s}\" for i in range(len(theta))]\nzs_aug = [f\"{z:d}, {deg:s}\" for z, deg in zip(zs, zs_deg)]\n\nA, P = dcsbm(zs, theta, B, return_prob=True)",
      "outputs": []
    },
    {
      "id": "5ebf3787",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.embed import node2vec_embed\nimport networkx as nx\np=1; q=10; T=200; r=500\nd = 4\n\nnp.random.seed(0)\nXhat1, _ = node2vec_embed(nx.from_numpy_array(A),\n                         return_hyperparameter=float(p), inout_hyperparameter=float(q),\n                         dimensions=d, num_walks=r, walk_length=T)",
      "outputs": []
    },
    {
      "id": "024086b7",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "p=1; q=1/10; T=200; r=500\nd = 4\n\nnp.random.seed(0)\nXhat2, _ = node2vec_embed(nx.from_numpy_array(A), return_hyperparameter=float(p), inout_hyperparameter=float(q),\n                   dimensions=d, num_walks=r, walk_length=T)",
      "outputs": []
    }
  ]
}