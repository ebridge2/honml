{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "id": "3bcaa84a",
      "cell_type": "markdown",
      "source": "(ch6:code_repr)=\n# Code Reproducibility",
      "metadata": {}
    },
    {
      "id": "a568235c",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\nfrom graphbook_code import dcsbm\n\nnk = 100  # 100 nodes per community\nK = 3  # the number of communities\nn = nk * K  # total number of nodes\n\nzs = np.repeat(np.arange(K)+1, repeats=nk)\n# block matrix and degree-correction factor\nB = np.array([[0.7, 0.2, 0.1], [0.2, 0.5, 0.1], [0.1, 0.1, 0.4]])\ntheta = np.tile(np.linspace(start=0, stop=1, num=nk), reps=K)\n# generate network sample\nnp.random.seed(0)\nA = dcsbm(zs, theta, B)\n\n# permute the nodes randomly\nvtx_perm = np.random.choice(n, size=n, replace=False)\nAperm = A[vtx_perm, :][:,vtx_perm]\nzperm = zs[vtx_perm]",
      "outputs": []
    },
    {
      "id": "d54da4e2",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import scipy as sp\nfrom graspologic.embed import AdjacencySpectralEmbed as ase\n\nXhat = ase(n_components=3, svd_seed=0).fit_transform(Aperm)\nD = sp.spatial.distance_matrix(Xhat, Xhat)",
      "outputs": []
    },
    {
      "id": "fb5a0450",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from sklearn.cluster import KMeans\n\nlabels_kmeans = KMeans(n_clusters = 3, random_state=0).fit_predict(Xhat)",
      "outputs": []
    },
    {
      "id": "d59ea9ae",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from sklearn.metrics import confusion_matrix\n\n# compute the confusion matrix between the true labels z\n# and the predicted labels labels_kmeans\ncf_matrix = confusion_matrix(zperm, labels_kmeans)",
      "outputs": []
    },
    {
      "id": "80c296df",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from sklearn.metrics import adjusted_rand_score\n\nari_kmeans = adjusted_rand_score(zperm, labels_kmeans)\nprint(ari_kmeans)\n# 0.490",
      "outputs": []
    },
    {
      "id": "b1f306d6",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.utils import remap_labels\n\nlabels_kmeans_remap = remap_labels(zperm, labels_kmeans)",
      "outputs": []
    },
    {
      "id": "ea6daf33",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# compute which assigned labels from labels_kmeans_remap differ from the true labels z\nerror = zperm - labels_kmeans_remap\n# if the difference between the community labels is non-zero, an error has occurred\nerror = error != 0\nerror_rate = np.mean(error)  # error rate is the frequency of making an error",
      "outputs": []
    },
    {
      "id": "e7d089d9",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "Xhat = ase(svd_seed=0).fit_transform(Aperm)\nprint(\"Estimated number of dimensions: {:d}\".format(Xhat.shape[1]))\n# Estimated number of dimensions: 3",
      "outputs": []
    },
    {
      "id": "fd89ecc1",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.cluster import KMeansCluster\n\nkm_clust = KMeansCluster(max_clusters = 10, random_state=0)\nlabels_kmclust = km_clust.fit_predict(Xhat)",
      "outputs": []
    },
    {
      "id": "a32da732",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import seaborn as sns\nfrom pandas import DataFrame as df\n\nnclusters = range(2, 11)  # graspologic nclusters goes from 2 to max_clusters\nsilhouette = km_clust.silhouette_ # obtain the respective silhouettes\n\n# place into pandas dataframe\nss_df = df({\"Number of Communities\": nclusters, \"Silhouette Score\": silhouette})\nsns.lineplot(data=ss_df, x=\"Number of Communities\", y=\"Silhouette Score\")",
      "outputs": []
    },
    {
      "id": "d63f0169",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\nfrom graspologic.simulations import sample_edges\nfrom graphbook_code import generate_sbm_pmtx\n    \ndef academic_pmtx(K, nk=10, return_zs=False):\n    \"\"\"\n    Produce probability matrix for academic example.\n    \"\"\"\n    n = K*nk\n    # get the community assignments\n    zs = np.repeat(np.arange(K)+1, repeats=nk)\n    # randomly generate proteges and lab leaders\n    unif_choices = np.random.uniform(size=n)\n    thetas = np.zeros(n)\n    # 90% are proteges\n    thetas[unif_choices > .1] = np.random.beta(1, 5, size=(unif_choices > .1).sum())\n    # 10% are lab leaders\n    thetas[unif_choices <= .1] = np.random.beta(2, 1, size=(unif_choices <= .1).sum())\n    # define block matrix\n    B = np.full(shape=(K,K), fill_value=0.01)\n    np.fill_diagonal(B, 1)\n    # generate probability matrix for SBM\n    Pp = generate_sbm_pmtx(zs, B)\n    Theta = np.diag(thetas)\n    # adjust probability matrix for SBM by degree-corrections\n    P = Theta @ Pp @ Theta.transpose()\n    if return_zs:\n        return P, zs\n    return P\n\ndef academic_example(K, nk=10, return_zs=False):\n    P = academic_pmtx(K, nk=nk, return_zs=return_zs)\n    if return_zs:\n        return (sample_edges(P[0]), P[1])\n    else:\n        return sample_edges(P)",
      "outputs": []
    },
    {
      "id": "2f4834f5",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import pandas as pd\nfrom tqdm import tqdm  # optional\n\nresults = []\nnrep = 50\nfor K in tqdm(np.linspace(start=2, stop=128, num=10, dtype=int)):\n    for j in range(nrep):\n        P = academic_pmtx(K)\n        n = P.shape[0]\n        results.append({\"Count\": np.triu(P, k=1).sum(), \"Edges\": \"Expected\", \n                        \"#Nodes\": n, \"Index\": j})\n        results.append({\"Count\": n*(n - 1)/2000, \"Edges\": \"Potential/1000\",\n                        \"#Nodes\": n, \"Index\": j})\n\ndf = pd.DataFrame(results)\ndf_mean=df.groupby([\"Edges\", \"#Nodes\"])[[\"Count\"]].mean()",
      "outputs": []
    },
    {
      "id": "7f2284b8",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "ax = sns.lineplot(data=df, x=\"#Nodes\", y=\"Count\", hue=\"Edges\")\nax.set(yscale=\"log\")",
      "outputs": []
    },
    {
      "id": "77477740",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "df_wide = pd.pivot(df_mean.reset_index(), index=\"#Nodes\", columns=\"Edges\", values=\"Count\")\n# remember normalizing constant of 100 for potential edges\ndf_wide[\"Density\"] = df_wide[\"Expected\"]/(1000*df_wide[\"Potential/1000\"])\ndf_wide = df_wide.reset_index()\n# plot it\nsns.lineplot(data=df_wide, x=\"#Nodes\", y=\"Density\", color=\"black\")",
      "outputs": []
    },
    {
      "id": "93e584d8",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "df_wide[\"Degree\"] = df_wide[\"Density\"]*(df_wide[\"#Nodes\"] - 1)\nsns.lineplot(data=df_wide, x=\"#Nodes\", y=\"Degree\", color=\"black\")",
      "outputs": []
    },
    {
      "id": "ef0dee81",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "np.random.seed(0)\nK = 10; nk = 100\nP, zs = academic_example(K, nk=nk, return_zs=True)\nA = sample_edges(P)\n\nprint(f\"# Non-zero entries: {A.sum().astype(int)}\")  \n# Non-zero entries: 5308\n\nprint(f\"# Number of entries: {A.size}\")  \n# Number of entries: 1000000",
      "outputs": []
    },
    {
      "id": "174b91f4",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "print(f\"Size in KB: {A.nbytes/1000:.3f} KB\")\n# Size in KB: 8000.000 KB\n\nB = A.astype(np.uint8)\nprint(f\"Size in KB: {B.nbytes/1000:.3f} KB\")\n# Size in KB: 1000.000 KB",
      "outputs": []
    },
    {
      "id": "fb24d7e5",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import scipy.sparse as sparse\n\nBtriu = sparse.triu(B)\nprint(f\"Size in KB: {Btriu.data.size/1000:.3f}\")\n# Size in KB: 2.654 KB",
      "outputs": []
    },
    {
      "id": "7a62db24",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "Btriu\n# <1000x1000 sparse matrix of type '<class 'numpy.uint8'>'\n#     with 2654 stored elements in COOrdinate format>",
      "outputs": []
    },
    {
      "id": "a77fdabf",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.utils import symmetrize\n\n# cast the sparse matrix back to a dense matrix,\n# and then triu symmetrize with graspologic\nA_new = symmetrize(Btriu.todense(), method=\"triu\")\nnp.array_equal(A_new, A)  # True",
      "outputs": []
    },
    {
      "id": "cc919811",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import time\nimport scipy as sp\n\n# a naive full svd on the dense matrix\ntimestart = time.time()\nU, S, Vh = sp.linalg.svd(A)\nXhat = U[:, 0:10] @ np.diag(np.sqrt(S[0:10]))\ntimeend = time.time()\nprint(f\"Naive approach: {timeend - timestart:3f} seconds\")\n# we get about 0.55 seconds\n\n# a sparse svd on the sparse matrix\nAcoo = sparse.coo_array(A)\ntimestart = time.time()\nU, S, Vh = sp.sparse.linalg.svds(Acoo, k=10)\nXhat = U @ np.diag(np.sqrt(S))\ntimeend = time.time()\nprint(f\"Sparse approach: {timeend-timestart:3f} seconds\")\n# we get about .01 seconds",
      "outputs": []
    },
    {
      "id": "e90fdd21",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "degrees = A.sum(axis=0)",
      "outputs": []
    },
    {
      "id": "560d14d3",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.utils import to_laplacian\nfrom graspologic.plot import pairplot\n\n# use sparse svd, so that we don't need to compute\n# 1000 singular vectors and can just calculate the top 10\nU, S, Vh = sp.sparse.linalg.svds(to_laplacian(A), k=10, random_state=0)\n# plot the first 4\npairplot(U[:,0:4], labels=zs, title=\"Eigenspokes in the Laplacian\")",
      "outputs": []
    },
    {
      "id": "a6dfb75b",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "U, S, Vh = sp.sparse.linalg.svds(A, k=10, random_state=0)\n# plot the first 4\nfig = pairplot(U[:,0:4], labels=zs, title=\"Eigenspokes in the adjacency matrix\")",
      "outputs": []
    },
    {
      "id": "684e7a4d",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "print(\"# Expected edges: {:.2f}\".format(np.triu(P).sum()))\n# Expected edges: 2654.00\nprint(\"# True edges: {:d}\".format(np.triu(A).sum().astype(int)))\n# True edges: 2654\nprint(\"# Potential edges: {:d}\".format(int(K*nk*(K*nk - 1)/2)))\n# Potential edges: 499500",
      "outputs": []
    },
    {
      "id": "c1c020ed",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\nfrom graphbook_code import siem\n\nn = 100\nZ = np.ones((n, n))\n\n# Fill the upper and lower 50th diagonals with 2\n# and the main diagonal with 0\nnp.fill_diagonal(Z[:, 50:], 2)\nnp.fill_diagonal(Z[50:, :], 2)\nnp.fill_diagonal(Z, 0)\n\np = [0.4, 0.6]\nnp.random.seed(0)\nA = siem(n, p, Z)",
      "outputs": []
    },
    {
      "id": "10f3842d",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "est_pvec = {k: A[Z == k].mean() for k in [1, 2]}\nprint(est_pvec)\n# {1: 0.3955102040816327, 2: 0.6}",
      "outputs": []
    },
    {
      "id": "5cca721c",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from scipy.stats import fisher_exact\nimport numpy as np\n\n# assemble the contingency table indicated\ntable = np.array([[3, 7], [7, 3]])\n_, pvalue = fisher_exact(table)\nprint(f\"p-value: {pvalue:.3f}\")\n# p-value: 0.179",
      "outputs": []
    },
    {
      "id": "a4b38951",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# compute an upper-triangular mask to only look at the\n# upper triangle since the network is simple (undirected and loopless)\nupper_tri_mask = np.triu(np.ones(A.shape), k=1).astype(bool)\ncolumn_clust1 = [\n    A[(Z == 1) & upper_tri_mask].sum(),\n    (A[(Z == 1) & upper_tri_mask] == 0).sum(),\n]\ncolumn_clust2 = [\n    A[(Z == 2) & upper_tri_mask].sum(),\n    (A[(Z == 2) & upper_tri_mask] == 0).sum(),\n]\ncont_tabl = np.vstack((column_clust1, column_clust2))",
      "outputs": []
    },
    {
      "id": "98125eb5",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "_, pvalue = fisher_exact(cont_tabl)\nprint(f\"p-value: {pvalue:.5f}\")\n# p-value: 0.00523",
      "outputs": []
    },
    {
      "id": "1f5525b1",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\nfrom graspologic.simulations import sbm\n\nnk = 50  # 50 nodes per community\nK = 2  # the number of communities\nn = nk * K  # total number of nodes\n\nzs = np.repeat(np.arange(1, K+1), repeats=nk)\n# block matrix\nB = np.array([[0.6, 0.3],[0.3, 0.5]])\n# generate network sample\nnp.random.seed(0)\nA = sbm([nk, nk], B)",
      "outputs": []
    },
    {
      "id": "760e8e81",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.models import SBMEstimator\n\n# instantiate the class object and fit\nmodel = SBMEstimator(directed=False, loops=False)\nmodel.fit(A, y=zs)\n# obtain the estimate of the block matrix\nBhat = model.block_p_",
      "outputs": []
    },
    {
      "id": "dfa2ad39",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# upper left has a value of 1, lower right has a value of 2,\n# and upper right, bottom left have a value of 3\nZ = np.array(zs).reshape(n, 1) @ np.array(zs).reshape(1, n)\n# make lower right have a value of 3\nZ[Z == 4] = 3",
      "outputs": []
    },
    {
      "id": "5b9b19ce",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import statsmodels.api as sm\nimport pandas as pd\nimport statsmodels.formula.api as smf\nfrom scipy import stats as spstat\n\n# upper triangle since the network is simple (undirected and loopless)\nupper_tri_non_diag = np.triu(np.ones(A.shape), k=1).astype(bool)\n\ndf_H1 = pd.DataFrame({\"Value\" : A[upper_tri_non_diag],\n            \"Group\": (Z[upper_tri_non_diag] != 2).astype(int)})",
      "outputs": []
    },
    {
      "id": "3697e643",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# fit the logistic regression model, regressing the outcome (edge or no edge)\n# onto the edge group (on-diagonal or off-diagonal), the grouping\n# corresponding to H1\nmodel_H1 = smf.logit(\"Value ~ C(Group)\", df_H1).fit()\n\n# compare the likelihood ratio statistic to the chi2 distribution\n# with 1 dof to see the fraction that is less than l1\ndof = 1\nprint(f\"p-value: {spstat.chi2.sf(model_H1.llr, dof):.3f}\")\n# p-value: 0.00000",
      "outputs": []
    },
    {
      "id": "11d7d7bc",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "df_H2 = pd.DataFrame({\"Value\": A[upper_tri_non_diag],\n                      \"Group\": Z[upper_tri_non_diag].astype(int)})\nmodel_H2 = smf.logit(\"Value ~ C(Group)\", df_H2).fit()\nlr_stat_H2vsH1 = model_H2.llr - model_H1.llr\nprint(f\"p-value: {spstat.chi2.sf(lr_stat_H2vsH1, 1):.7f}\")\n# 0.00008",
      "outputs": []
    },
    {
      "id": "78398096",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\nfrom graspologic.simulations import sbm\n\n# first 100 nodes are traffickers, second 900 are non-traffickers\nns = [100, 900]\nB = np.array([[0.3, 0.1], [0.1, 0.2]])\nnp.random.seed(0)\nA = sbm(ns, B)",
      "outputs": []
    },
    {
      "id": "556e3bdb",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# the number of seed nodes\nnseeds = 20\n# The first ns[0] nodes are the human traffickers, so choose 20 seeds\n# at random\nseed_ids = np.random.choice(ns[0], size=20)",
      "outputs": []
    },
    {
      "id": "e4b45458",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.embed import AdjacencySpectralEmbed as ase\n\nXhat = ase(n_components=2, svd_seed=0).fit_transform(A)",
      "outputs": []
    },
    {
      "id": "597b77a8",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from sklearn.cluster import KMeans\n\n# community detection with kmeans\nkm_clust = KMeans(n_clusters=2, random_state=0)\nkm_clust.fit(Xhat)\nlabels_kmeans = km_clust.fit_predict(Xhat)",
      "outputs": []
    },
    {
      "id": "8c5b925e",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graphbook_code import ohe_comm_vec\n\n# estimated community assignment matrix\nChat = ohe_comm_vec(labels_kmeans)\n\n# get the community (class) with the most seeds\ncomm_of_seeds = np.argmax(Chat[seed_ids,:].sum(axis=0))\n\n# get centroid of the community that seeds tend to be\n# assigned to\ncentroid_seeds = km_clust.cluster_centers_[comm_of_seeds]",
      "outputs": []
    },
    {
      "id": "df6b57f1",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from scipy.spatial.distance import cdist\nfrom scipy.stats import rankdata\n\n# compute the distance to the centroid for all estimated latent positions\ndists_to_centroid = cdist(Xhat, centroid_seeds.reshape(1, -1)).reshape(-1)\n# compute the node numbers for all the nonseed nodes\nnonseed_bool = np.ones((np.sum(ns)))\nnonseed_bool[seed_ids] = 0\nnonseed_ids = np.array(np.where(nonseed_bool)).reshape(-1)\n\n# isolate the distances to the centroid for the nonseed nodes\nnonseed_dists = dists_to_centroid[nonseed_ids]",
      "outputs": []
    },
    {
      "id": "45fffba0",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# produce the nomination list\nnom_list_nonseeds = np.argsort(nonseed_dists).reshape(-1)\n# obtain a nomination list in terms of the original node ids\nnom_list = nonseed_ids[nom_list_nonseeds]",
      "outputs": []
    },
    {
      "id": "4d80de14",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import numpy as np\nfrom graspologic.simulations import sbm\n\n# the in-sample nodes\nn = 100\nnk = 50\n# the out-of-sample nodes\nnp1 = 1; np2 = 2\nB = np.array([[0.6, 0.2], [0.2, 0.4]])\n# sample network\nnp.random.seed(0)\nA, zs = sbm([nk + np1, nk + np2], B, return_labels=True)",
      "outputs": []
    },
    {
      "id": "3a2a5657",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.utils import remove_vertices\n\n# the indices of the out-of-sample nodes\noos_idx = [nk, nk + np1 + nk, nk + np1 + nk + 1]\n# get adjacency matrix and the adjacency vectors A prime\nAin, Aoos = remove_vertices(A, indices=oos_idx, return_removed=True)",
      "outputs": []
    },
    {
      "id": "f4658b1f",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "from graspologic.embed import AdjacencySpectralEmbed as ase\n\noos_embedder = ase()\n# estimate latent positions for the in-sample nodes\n# using the subnetwork induced by the in-sample nodes\nXhat_in = oos_embedder.fit_transform(Ain)",
      "outputs": []
    },
    {
      "id": "9f408020",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "Xhat_oos = oos_embedder.transform(Aoos)",
      "outputs": []
    }
  ]
}